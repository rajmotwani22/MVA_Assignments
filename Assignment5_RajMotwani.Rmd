---
title: "Assignment5_Raj_Motwani"
author: "Raj Motwani"
date: "2024-03-06"
output: html_document
---


```{r}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)

# With made up data. 
base_data <- read.csv("/Users/raj/Desktop/Compressed_b_data.csv",row.names=1, fill = TRUE)
base_data


#colnames(base_data) <- rownames(base_data)
#base_data
base_data <- as.dist(base_data)
base_data

# Clustering
#Single
mat5.nn <- hclust(base_data, method = "single")
plot(mat5.nn, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Nearest neighbor linkage")

#Default - Complete
mat5.fn <- hclust(base_data)
plot(mat5.fn,hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Farthest neighbor linkage")

#Average
mat5.avl <- hclust(base_data,method="average")
plot(mat5.avl,hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Group average linkage")


# Lets use Canines

matstd.can <- scale(base_data)

# Creating a (Euclidean) distance matrix of the standardized data 
dist.canine <- dist(matstd.can, method="euclidean")

# Invoking hclust command (cluster analysis by single linkage method)      
cluscanine.nn <- hclust(dist.canine, method = "single") 

# Plotting vertical dendrogram      
# create extra margin room in the dendrogram, on the bottom (Canine species' labels)
#par(mar=c(6, 4, 4, 2) + 0.1)
plot(as.dendrogram(cluscanine.nn),ylab="Distance between Canine species",ylim=c(0,2.5),main="Dendrogram of six canine species")


# Euro base_datament Data 

base_data <- read.csv("/Users/raj/Desktop/Compressed_b_data.csv",row.names=1, fill = TRUE)
attach(base_data)
dim(base_data)
str(base_data)
#base_data$Category <- as.factor(base_data$Category)
str(base_data)
# Hirerarchic cluster analysis, Nearest-neighbor

# Standardizing the data with scale()

matstd.base_data <- scale(base_data[,2:6])
# Creating a (Euclidean) distance matrix of the standardized data
dist.base_data <- dist(matstd.base_data, method="euclidean")
# Invoking hclust command (cluster analysis by single linkage method)
clusbase_data.nn <- hclust(dist.base_data, method = "single")

plot(as.dendrogram(clusbase_data.nn),ylab="Distance between countries",ylim=c(0,6),
     main="Dendrogram. People base_dataed in nine industry groups \n  from European countries")

plot(as.dendrogram(clusbase_data.nn), xlab= "Distance between countries", xlim=c(6,0),
     horiz = TRUE,main="Dendrogram. People base_dataed in nine industry groups from European countries")


(agn.base_data <- agnes(base_data, metric="euclidean", stand=TRUE, method = "single"))
#View(agn.base_data)

#  Description of cluster merging
agn.base_data$merge

#Dendogram
plot(as.dendrogram(agn.base_data), xlab= "Distance between Countries",xlim=c(8,0),
     horiz = TRUE,main="Dendrogram \n base_datament in nine industry groups in European countries")

#Interactive Plots
#plot(agn.base_data,ask=TRUE)
plot(agn.base_data, which.plots=1)
plot(agn.base_data, which.plots=2)
plot(agn.base_data, which.plots=3)

# K-Means Clustering

matstd.base_data <- scale(base_data[,2:6])
# K-means, k=2, 3, 4, 5, 6
# 5 clusters are chosen
matstd.base_data
(kmeans2.base_data <- kmeans(matstd.base_data,2,nstart =10))
# Computing the percentage of variation accounted for. Two clusters
perc.var.2 <- round(100*(1 - kmeans2.base_data$betweenss/kmeans2.base_data$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2

# Computing the percentage of variation accounted for. Three clusters
(kmeans3.base_data <- kmeans(matstd.base_data,3,nstart =10))
perc.var.3 <- round(100*(1 - kmeans3.base_data$betweenss/kmeans3.base_data$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3

# Computing the percentage of variation accounted for. Four clusters
(kmeans4.base_data <- kmeans(matstd.base_data,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.base_data$betweenss/kmeans4.base_data$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4

# Computing the percentage of variation accounted for. Five clusters
(kmeans5.base_data <- kmeans(matstd.base_data,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.base_data$betweenss/kmeans5.base_data$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5

#For each model, decide the optimal number of clusters and explain why.
#We are taking 4 clusters because the number  Elbow point at k = 4, suggesting a reasonable choice for the number of clusters.


Variance_List <- c(perc.var.2,perc.var.3,perc.var.4,perc.var.5)

Variance_List
plot(Variance_List)
#
# Saving four k-means clusters in a list
clus1 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 1]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 1]))
colnames(clus1) <- "Cluster 1"


clus2 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 2]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 2]))
colnames(clus2) <- "Cluster 2"
clus3 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 3]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 3]))
colnames(clus3) <- "Cluster 3"
clus4 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 4]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 4]))
colnames(clus4) <- "Cluster 4"
list(clus1,clus2,clus3,clus4)
detach(base_data)

# gg Visualizations with new Dataset

# Assuming you have the cluster assignments from kmeans4.base_data$cluster
cluster_assignments <- kmeans4.base_data$cluster

# Display the count of individuals in each cluster
table(cluster_assignments)

# Assuming 'df' is your original data frame
df_with_clusters <- cbind( Cluster = cluster_assignments)

# Display the data frame with cluster assignments
head(df_with_clusters)


# Visualize
#fviz_nbclust(res.nbclust, ggtheme = theme_minimal())

# Quality of Clustering

set.seed(123)
# Enhanced hierarchical clustering, cut in 3 groups
res.hc <- base_data[, -1] %>% scale() %>%
  eclust("hclust", k = 2, graph = FALSE)

# Visualize with factoextra
fviz_dend(res.hc, palette = "jco",
          rect = TRUE, show_labels = FALSE)


#Inspect the silhouette plot:
fviz_silhouette(res.hc)

# Silhouette width of observations
sil <- res.hc$silinfo$widths[, 1:3]
#Silhouette Method:
#Optimal Number of Clusters: Silhouette method confirms k=4 as the number that maximizes the average silhouette width.

# There are no negatives in my data
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]

#Show the membership for each cluster (4 points)
# Assuming 'df' is your original data frame
df_with_clusters <- cbind( Cluster = kmeans4.base_data$cluster)
# Display the data frame with cluster assignments
head(df_with_clusters)


#show a visualization of the cluster and membership using the first two Principal Components (2 points)
# Assuming you have the cluster assignments from kmeans4.base_data$cluster
cluster_assignments <- kmeans4.base_data$cluster

# Assuming 'df' is your original data frame
df_with_clusters <- cbind( Cluster = cluster_assignments)

# Perform PCA
pca_result <- prcomp(matstd.base_data)

# Extract the first two principal components
pc1 <- pca_result$x[, 1]
pc2 <- pca_result$x[, 2]

# Scatter plot with color-coded clusters
plot(pc1, pc2, col = cluster_assignments, pch = 16, main = "PCA of Clusters", xlab = "PC1", ylab = "PC2")
legend("topright", legend = unique(cluster_assignments), col = unique(cluster_assignments), pch = 16, title = "Cluster")

```
